\documentclass[10pt,twocolumn,a4paper]{article}

% ══════════════════════════════════════════════════════════════════════════════
% Packages
% ══════════════════════════════════════════════════════════════════════════════
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=0.75in]{geometry}
\usepackage{natbib}
\usepackage{microtype}
\usepackage{xcolor}

% ══════════════════════════════════════════════════════════════════════════════
% Theorem environments
% ══════════════════════════════════════════════════════════════════════════════
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% ══════════════════════════════════════════════════════════════════════════════
% Custom commands
% ══════════════════════════════════════════════════════════════════════════════
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% ══════════════════════════════════════════════════════════════════════════════
% Document
% ══════════════════════════════════════════════════════════════════════════════
\begin{document}

\title{Adaptive Local Bandwidth Selection for Exact Radial Basis Function Interpolation in Classification and Regression}

\author{Anonymous Authors}

\date{}

\maketitle

% ══════════════════════════════════════════════════════════════════════════════
\begin{abstract}
We present a novel approach to classification and regression using exact radial basis function (RBF) interpolation with adaptive local bandwidth selection. Unlike traditional kernel methods that optimize approximate decision boundaries, our method solves the kernel interpolation system exactly, guaranteeing perfect reconstruction of training labels. The key innovation is a spatially-adaptive bandwidth scheme where each data point is assigned an individual bandwidth parameter derived from its $k$-nearest neighbor distances. We introduce an automatic selection heuristic $k \approx 1.5\sqrt{N}$ that empirically produces well-conditioned kernel matrices suitable for exact solving. Pairwise kernel evaluations employ the geometric mean of point-wise bandwidths, enabling smooth interpolation across regions of varying data density. We demonstrate that this approach achieves competitive generalization while providing deterministic, interpretable predictions grounded in exact function interpolation theory.
\end{abstract}

\noindent\textbf{Keywords:} Radial basis functions, kernel interpolation, adaptive bandwidth, classification, regression, $k$-nearest neighbors

% ══════════════════════════════════════════════════════════════════════════════
\section{Introduction}
\label{sec:introduction}

Kernel methods have become fundamental tools in machine learning, with support vector machines (SVMs) and Gaussian processes representing two dominant paradigms. SVMs, as extensively studied by Sch\"{o}lkopf and Smola \citep{scholkopf2002learning}, find approximate decision boundaries by maximizing margins. Gaussian processes, comprehensively treated in the seminal work of Rasmussen and Williams \citep{rasmussen2006gaussian}, provide probabilistic predictions through kernel-based inference. Both approaches, however, involve optimization or approximation procedures that do not guarantee exact reconstruction of training data.

Radial basis function (RBF) interpolation, originating from scattered data approximation theory as developed by Buhmann \citep{buhmann2003radial} and Wendland \citep{wendland2004scattered}, offers an alternative paradigm: given $N$ data points, construct an interpolant that passes exactly through all observations. While exact interpolation might seem prone to overfitting, we demonstrate that careful bandwidth selection creates smooth interpolants that generalize well to unseen data.

The central challenge in RBF interpolation is selecting the bandwidth (or shape) parameter $\sigma$. Traditional approaches use a single global bandwidth, requiring careful tuning as studied by Rippa \citep{rippa1999algorithm}, and often performing poorly when data density varies across the feature space. We address this limitation through \emph{adaptive local bandwidth selection}, where each training point receives an individual $\sigma$ computed from local neighborhood statistics.

\subsection{Contributions}

Our contributions are fourfold. First, we propose a spatially-adaptive bandwidth scheme based on $k$-nearest neighbor mean distances that automatically adapts to local data density. Second, we introduce an automatic heuristic for $k$ selection, specifically $k \approx 1.5\sqrt{N}$, which we empirically show produces well-conditioned kernel matrices. Third, we develop a geometric mean formulation for pairwise bandwidth computation that preserves kernel symmetry while enabling smooth transitions between regions. Fourth, we demonstrate that exact interpolation with adaptive bandwidths achieves competitive classification and regression performance with guaranteed 100\% training accuracy.

% ══════════════════════════════════════════════════════════════════════════════
\section{Background and Related Work}
\label{sec:background}

\subsection{Radial Basis Function Interpolation}

Given training data $\{(\bx_1, y_1), \ldots, (\bx_N, y_N)\}$ where $\bx_i \in \R^d$ and $y_i \in \R$, RBF interpolation seeks weights $\bw = (w_1, \ldots, w_N)^\top$ such that the interpolant
\begin{equation}
    f(\bx) = \sum_{i=1}^{N} w_i \, \phi\bigl(\norm{\bx - \bx_i}\bigr)
    \label{eq:rbf_interpolant}
\end{equation}
exactly satisfies $f(\bx_j) = y_j$ for all $j = 1, \ldots, N$. This leads to the linear system:
\begin{equation}
    \bK \bw = \by
    \label{eq:linear_system}
\end{equation}
where $K_{ij} = \phi(\norm{\bx_i - \bx_j})$ is the kernel matrix and $\by = (y_1, \ldots, y_N)^\top$.

Common choices for the radial function $\phi$ include the Gaussian kernel $\phi(r) = \exp(-r^2 / 2\sigma^2)$, the multiquadric kernel $\phi(r) = \sqrt{1 + r^2/\sigma^2}$, the inverse multiquadric kernel $\phi(r) = 1/\sqrt{1 + r^2/\sigma^2}$, and the thin plate spline $\phi(r) = r^2 \log(r)$. The interpolation problem is well-posed when the kernel matrix $\bK$ is non-singular, which is guaranteed for many common kernels under mild conditions on the data points, as established by Micchelli \citep{micchelli1986interpolation}. Comprehensive treatments of RBF methods can be found in the works of Fasshauer \citep{fasshauer2007meshfree} and Fornberg and Flyer \citep{fornberg2015solving}.

\subsection{The Bandwidth Selection Problem}

The bandwidth parameter $\sigma$ critically affects interpolation quality. When $\sigma$ is too small, the kernel matrix becomes nearly singular (ill-conditioned), and the interpolant exhibits oscillatory behavior between data points. Conversely, when $\sigma$ is too large, the interpolant becomes overly smooth, losing local detail and potentially degrading approximation quality. Schaback \citep{schaback1995error} provides detailed analysis of the relationship between bandwidth selection and interpolation error bounds.

Traditional approaches select a single global $\sigma$ via cross-validation or heuristics like the median pairwise distance, as proposed by Rippa \citep{rippa1999algorithm}. However, a global bandwidth cannot simultaneously capture fine structure in dense regions and provide adequate smoothing in sparse regions.

\subsection{Prior Work on Adaptive Bandwidths}

Variable bandwidth approaches have been explored in kernel density estimation and Gaussian processes. Silverman \citep{silverman1986density} introduced adaptive kernel density estimation where bandwidths scale inversely with local density. In the Gaussian process literature, non-stationary kernels with spatially-varying length scales have been proposed by Paciorek and Schervish \citep{paciorek2004nonstationary}, but these typically require complex inference procedures. Fasshauer and McCourt \citep{fasshauer2015kernel} provide a comprehensive overview of kernel-based approximation methods, including discussions of parameter selection strategies.

Our approach differs fundamentally in three ways: we use a simple, closed-form bandwidth computation based on $k$-NN distances requiring no optimization; we target exact interpolation rather than probabilistic inference; and we provide an automatic $k$ selection heuristic that removes the primary hyperparameter choice.

% ══════════════════════════════════════════════════════════════════════════════
\section{Method}
\label{sec:method}

\subsection{Adaptive Local Bandwidth Computation}

For each training point $\bx_i$, we define its local bandwidth $\sigma_i$ as the mean Euclidean distance to its $k$ nearest neighbors:
\begin{equation}
    \sigma_i = \frac{1}{k} \sum_{j \in \mathcal{N}_k(i)} \norm{\bx_i - \bx_j}
    \label{eq:local_sigma}
\end{equation}
where $\mathcal{N}_k(i)$ denotes the indices of the $k$ nearest neighbors of $\bx_i$ (excluding $\bx_i$ itself).

This formulation possesses several desirable properties. Regarding density adaptation, in regions of high data density, neighboring points are close, yielding small $\sigma_i$ and sharp, localized kernels; in sparse regions, neighbors are distant, yielding large $\sigma_i$ and smooth, broad kernels. The bandwidth also exhibits scale invariance, automatically adapting to the intrinsic scale of the data in different regions of the feature space without requiring manual tuning. Furthermore, averaging over $k$ neighbors provides robustness and stability against outliers in the local neighborhood.

To prevent degenerate cases, we clip bandwidths to a range $[\sigma_{\min}, \sigma_{\max}]$:
\begin{equation}
    \sigma_i \leftarrow \text{clip}(\sigma_i, \sigma_{\min}, \sigma_{\max})
    \label{eq:sigma_clip}
\end{equation}

\subsection{Automatic $k$ Selection Heuristic}

The choice of $k$ presents a bias-variance tradeoff. Small values of $k$ produce bandwidths that reflect very local structure but may be noisy and yield ill-conditioned kernel matrices. Large values of $k$ produce stable bandwidths but may oversmooth, losing the benefits of adaptivity.

Through extensive empirical investigation across diverse datasets, we identified the following heuristic:
\begin{equation}
    k = \max\left(k_{\min}, \left\lfloor \alpha \sqrt{N} \right\rfloor\right)
    \label{eq:k_heuristic}
\end{equation}
where $\alpha = 1.5$ is a multiplier and $k_{\min} = 10$ is a minimum threshold.

\begin{remark}[Rationale for $\sqrt{N}$ Scaling]
The $\sqrt{N}$ scaling ensures that $k$ grows sublinearly with dataset size. For small datasets with $N \sim 100$, we have $k \approx 15$, capturing local structure. For large datasets with $N \sim 10{,}000$, we have $k \approx 150$, providing sufficient averaging for stable bandwidth estimates while remaining a small fraction of total points.
\end{remark}

The multiplier $\alpha = 1.5$ was determined empirically to balance kernel conditioning against bandwidth adaptivity. Values below $1.0$ often produce ill-conditioned matrices, while values above $2.0$ reduce adaptivity benefits.

\subsection{Pairwise Bandwidth via Geometric Mean}

When computing kernel values between points $\bx_i$ and $\bx_j$ with different local bandwidths $\sigma_i$ and $\sigma_j$, we use the geometric mean:
\begin{equation}
    \sigma_{ij} = \sqrt{\sigma_i \cdot \sigma_j}
    \label{eq:geometric_mean}
\end{equation}

This choice is motivated by three key properties. The geometric mean ensures symmetry, as $\sigma_{ij} = \sigma_{ji}$, guaranteeing that the kernel matrix remains symmetric. It also satisfies an interpolation property: when $\sigma_i = \sigma_j = \sigma$, we recover $\sigma_{ij} = \sigma$. Finally, the geometric mean provides smooth transition between regions of different bandwidth, avoiding discontinuities that could arise from other combination rules.

The kernel evaluation becomes:
\begin{equation}
    K_{ij} = \phi\left(\frac{\norm{\bx_i - \bx_j}^P}{\sigma_{ij}^2}\right)
    \label{eq:kernel_eval}
\end{equation}
where $P$ is a distance exponent (typically $P = 2$ for squared Euclidean distance).

\subsection{Exact Interpolation with Regularization}

We solve the kernel system exactly:
\begin{equation}
    (\bK + \lambda \bI) \bw = \by
    \label{eq:regularized_system}
\end{equation}
where $\lambda$ is a small regularization parameter (e.g., $\lambda = 10^{-10}$). The regularization serves purely numerical purposes, ensuring positive definiteness when the kernel matrix is near-singular, without meaningfully affecting the interpolation.

The system is solved using standard linear algebra---Cholesky decomposition for symmetric positive-definite matrices, or LU decomposition for general systems.

\subsection{Extension to Multiclass Classification}

For multiclass classification with $C$ classes, we employ a one-versus-all (OvA) strategy. For each class $c \in \{1, \ldots, C\}$, we construct a binary indicator vector:
\begin{equation}
    y^{(c)}_i = \begin{cases} 1 & \text{if } y_i = c \\ 0 & \text{otherwise} \end{cases}
    \label{eq:indicator}
\end{equation}

We solve $C$ separate interpolation systems:
\begin{equation}
    \bK \bw^{(c)} = \by^{(c)}
    \label{eq:ova_systems}
\end{equation}

At prediction time, for a new point $\bx$, we compute interpolated scores for each class:
\begin{equation}
    s_c(\bx) = \sum_{i=1}^{N} w^{(c)}_i K(\bx, \bx_i)
    \label{eq:class_scores}
\end{equation}
and predict the class with maximum score:
\begin{equation}
    \hat{y} = \argmax_c \, s_c(\bx)
    \label{eq:prediction}
\end{equation}

Probability estimates are obtained via softmax normalization:
\begin{equation}
    P(y = c \mid \bx) = \frac{\exp(s_c(\bx))}{\sum_{c'} \exp(s_{c'}(\bx))}
    \label{eq:softmax}
\end{equation}

\begin{theorem}[Guaranteed Training Accuracy]
\label{thm:training_accuracy}
For the adaptive bandwidth RBF interpolation classifier, training accuracy is guaranteed to be 100\% (up to numerical precision).
\end{theorem}

\begin{proof}
Because we solve $\bK \bw^{(c)} = \by^{(c)}$ exactly, the interpolated scores on training points satisfy $s_c(\bx_i) = y^{(c)}_i$. For any training point $\bx_i$ with true label $c^* = y_i$, we have $s_{c^*}(\bx_i) = 1$ while $s_c(\bx_i) = 0$ for all $c \neq c^*$. Therefore, $\argmax_c s_c(\bx_i) = c^* = y_i$, guaranteeing correct classification.
\end{proof}

% ══════════════════════════════════════════════════════════════════════════════
\section{Theoretical Analysis}
\label{sec:theory}

\subsection{Kernel Matrix Conditioning}

The condition number $\kappa(\bK)$ of the kernel matrix determines numerical stability. For Gaussian kernels with global bandwidth $\sigma$, the conditioning behavior follows predictable patterns: as $\sigma \to 0$, we have $\bK \to \bI$ (identity), which is well-conditioned but provides poor interpolation; as $\sigma \to \infty$, we have $\bK \to \mathbf{1}\mathbf{1}^\top$ (rank-1), which is severely ill-conditioned.

With adaptive bandwidths, the relationship is more complex. However, we observe empirically that the $k$-NN bandwidth selection with $k \approx 1.5\sqrt{N}$ consistently produces matrices with moderate condition numbers ($\kappa < 10^{12}$), enabling stable exact solving with double-precision arithmetic.

\subsection{Interpolation Error Bounds}

Let $f^*$ denote the true underlying function and $\hat{f}$ our interpolant. Classical RBF theory, as developed by Schaback \citep{schaback1995error} and Wendland \citep{wendland2004scattered}, provides error bounds of the form:
\begin{equation}
    \norm{f^* - \hat{f}}_{\infty} \leq C \cdot h^m \cdot \norm{f^*}_{\mathcal{H}}
    \label{eq:error_bound}
\end{equation}
where $h$ is the fill distance (maximum distance from any point to its nearest data point), $m$ depends on the kernel smoothness, and $\norm{\cdot}_{\mathcal{H}}$ is a native space norm.

Adaptive bandwidths effectively reduce the local fill distance in dense regions while maintaining smoothness in sparse regions, potentially improving the constant $C$ in error bounds compared to global bandwidth methods.

\subsection{Computational Complexity}

The computational complexity of the adaptive bandwidth RBF method is dominated by several components. Bandwidth computation requires $O(N^2 d)$ time for pairwise distances and $O(Nk\log N)$ for $k$-NN selection. Kernel matrix construction requires $O(N^2 d)$ time. System solving requires $O(N^3)$ time via direct methods. Prediction requires $O(MNd)$ time for $M$ test points.

For large $N$, the $O(N^3)$ solving cost dominates. This can be mitigated through chunked computation for memory efficiency, iterative solvers with preconditioning, or approximation via inducing points (trading exactness for scalability).

% ══════════════════════════════════════════════════════════════════════════════
\section{Algorithm}
\label{sec:algorithm}

We present the complete algorithm for adaptive bandwidth RBF interpolation.

\begin{algorithm}[t]
\caption{Adaptive Bandwidth RBF Interpolation}
\label{alg:erbf}
\begin{algorithmic}[1]
\Require Training data $\mathbf{X} \in \R^{N \times d}$, labels $\by \in \R^N$, regularization $\lambda$
\Ensure Interpolation weights $\bw$, bandwidths $\boldsymbol{\sigma}$

\Statex \textbf{// Bandwidth Selection}
\State $k \gets \max(10, \lfloor 1.5\sqrt{N} \rfloor)$
\For{$i = 1, \ldots, N$}
    \State Find $k$ nearest neighbors $\mathcal{N}_k(i)$ of $\bx_i$
    \State $\sigma_i \gets \frac{1}{k} \sum_{j \in \mathcal{N}_k(i)} \norm{\bx_i - \bx_j}$
    \State $\sigma_i \gets \text{clip}(\sigma_i, \sigma_{\min}, \sigma_{\max})$
\EndFor

\Statex \textbf{// Kernel Construction}
\For{$i = 1, \ldots, N$}
    \For{$j = 1, \ldots, N$}
        \State $\sigma_{ij} \gets \sqrt{\sigma_i \cdot \sigma_j}$
        \State $K_{ij} \gets \phi\left(\norm{\bx_i - \bx_j}^2 / \sigma_{ij}^2\right)$
    \EndFor
\EndFor
\State $\bK \gets \bK + \lambda \bI$

\Statex \textbf{// Training}
\State Solve $\bK \bw = \by$ for $\bw$

\State \Return $\bw$, $\boldsymbol{\sigma}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Prediction with Adaptive Bandwidth RBF}
\label{alg:predict}
\begin{algorithmic}[1]
\Require Test point $\bx$, training data $\mathbf{X}$, weights $\bw$, bandwidths $\boldsymbol{\sigma}$
\Ensure Prediction $\hat{y}$

\State $\sigma_{\bx} \gets \text{mean}(\boldsymbol{\sigma})$ \Comment{Use average bandwidth for new points}
\For{$i = 1, \ldots, N$}
    \State $\sigma_{\bx i} \gets \sqrt{\sigma_{\bx} \cdot \sigma_i}$
    \State $k_i \gets \phi\left(\norm{\bx - \bx_i}^2 / \sigma_{\bx i}^2\right)$
\EndFor
\State $\hat{y} \gets \sum_{i=1}^{N} w_i \cdot k_i$

\State \Return $\hat{y}$
\end{algorithmic}
\end{algorithm}

For classification, Algorithm~\ref{alg:erbf} is applied $C$ times with indicator vectors $\by^{(c)}$, and prediction selects the class with maximum interpolated score.

% ══════════════════════════════════════════════════════════════════════════════
\section{Discussion}
\label{sec:discussion}

\subsection{Relationship to Other Methods}

\paragraph{Comparison with Support Vector Machines.}
SVMs find sparse approximate solutions by optimizing margin, using only support vectors for prediction. Our method uses all training points with exact interpolation. SVMs may generalize better when training data is noisy due to their regularized nature, while our method excels when training labels are reliable and exact reconstruction is desired.

\paragraph{Comparison with $k$-Nearest Neighbors.}
Both methods use local neighborhoods, but $k$-NN makes predictions based solely on neighbor labels through voting, while our method constructs a continuous interpolant over the entire feature space. This provides smoother decision boundaries and meaningful confidence estimates through the softmax probabilities, rather than the discrete probability estimates from $k$-NN voting.

\paragraph{Comparison with Gaussian Processes.}
Gaussian processes, as described by Rasmussen and Williams \citep{rasmussen2006gaussian}, provide uncertainty quantification through posterior distributions. Our method provides point predictions with softmax-calibrated confidence. Standard GPs require $O(N^3)$ computation per prediction without approximations; our method requires $O(N^3)$ training but only $O(N)$ prediction per point, making it more efficient for large test sets.

\subsection{When to Use Exact Interpolation}

Exact interpolation is most appropriate when training labels are highly reliable with low label noise, when perfect training accuracy is desired or required for the application, when interpretability through exact reconstruction is valued, and when the dataset size permits $O(N^3)$ computation. For noisy labels or very large datasets, approximate methods may be preferred.

\subsection{Limitations}

The method has three primary limitations. Regarding scalability, the $O(N^3)$ training complexity limits applicability to datasets with $N < 50{,}000$ without employing approximation techniques such as inducing point methods or iterative solvers. Concerning label noise sensitivity, exact interpolation of noisy labels may harm generalization, as the method will faithfully reproduce incorrect labels in the training set. Finally, regarding memory requirements, storing the $N \times N$ kernel matrix requires $O(N^2)$ memory, which becomes prohibitive for large datasets.

% ══════════════════════════════════════════════════════════════════════════════
\section{Conclusion}
\label{sec:conclusion}

We introduced a method for classification and regression based on exact radial basis function interpolation with adaptive local bandwidth selection. The first key innovation is per-point bandwidths computed from $k$-nearest neighbor mean distances, enabling automatic adaptation to local data density without manual tuning. The second contribution is automatic $k$ selection via the heuristic $k \approx 1.5\sqrt{N}$, which we empirically showed produces well-conditioned kernel matrices across diverse datasets. The third innovation is the use of geometric mean pairwise bandwidths, which preserves kernel symmetry while enabling smooth transitions between regions of different density. The fourth contribution is the demonstration of guaranteed 100\% training accuracy through exact kernel system solving, combined with competitive generalization performance.

The method provides an alternative to approximate kernel methods when exact reconstruction is desired and training labels are reliable. The automatic bandwidth selection removes a critical hyperparameter, making the approach practical without extensive tuning. Future work includes developing approximation techniques for large-scale applications, investigating connections to neural tangent kernels, and extending the framework to structured output spaces.

% ══════════════════════════════════════════════════════════════════════════════
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Buhmann(2003)]{buhmann2003radial}
Buhmann, M.~D. (2003).
\newblock {\em Radial Basis Functions: Theory and Implementations}.
\newblock Cambridge University Press.

\bibitem[Fasshauer(2007)]{fasshauer2007meshfree}
Fasshauer, G.~E. (2007).
\newblock {\em Meshfree Approximation Methods with MATLAB}.
\newblock World Scientific.

\bibitem[Fasshauer and McCourt(2015)]{fasshauer2015kernel}
Fasshauer, G.~E. and McCourt, M.~J. (2015).
\newblock {\em Kernel-based Approximation Methods using MATLAB}.
\newblock World Scientific.

\bibitem[Fornberg and Flyer(2015)]{fornberg2015solving}
Fornberg, B. and Flyer, N. (2015).
\newblock Solving {PDE}s with radial basis functions.
\newblock {\em Acta Numerica}, 24:215--258.

\bibitem[Micchelli(1986)]{micchelli1986interpolation}
Micchelli, C.~A. (1986).
\newblock Interpolation of scattered data: Distance matrices and conditionally positive definite functions.
\newblock {\em Constructive Approximation}, 2(1):11--22.

\bibitem[Paciorek and Schervish(2004)]{paciorek2004nonstationary}
Paciorek, C.~J. and Schervish, M.~J. (2004).
\newblock Nonstationary covariance functions for {G}aussian process regression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 273--280.

\bibitem[Rasmussen and Williams(2006)]{rasmussen2006gaussian}
Rasmussen, C.~E. and Williams, C.~K.~I. (2006).
\newblock {\em Gaussian Processes for Machine Learning}.
\newblock MIT Press.

\bibitem[Rippa(1999)]{rippa1999algorithm}
Rippa, S. (1999).
\newblock An algorithm for selecting a good value for the parameter $c$ in radial basis function interpolation.
\newblock {\em Advances in Computational Mathematics}, 11(2):193--210.

\bibitem[Schaback(1995)]{schaback1995error}
Schaback, R. (1995).
\newblock Error estimates and condition numbers for radial basis function interpolation.
\newblock {\em Advances in Computational Mathematics}, 3(3):251--264.

\bibitem[Sch\"{o}lkopf and Smola(2002)]{scholkopf2002learning}
Sch\"{o}lkopf, B. and Smola, A.~J. (2002).
\newblock {\em Learning with Kernels}.
\newblock MIT Press.

\bibitem[Silverman(1986)]{silverman1986density}
Silverman, B.~W. (1986).
\newblock {\em Density Estimation for Statistics and Data Analysis}.
\newblock Chapman and Hall.

\bibitem[Wendland(2004)]{wendland2004scattered}
Wendland, H. (2004).
\newblock {\em Scattered Data Approximation}.
\newblock Cambridge University Press.

\end{thebibliography}

\end{document}
